#!/usr/bin/env python3
"""
CX ëŒ€ì‹œë³´ë“œ Phase 3: cx-insight-store.db â†’ index.html ìë™ ì£¼ì…

ì‚¬ìš©ë²•:
  python3 scripts/phase3_db_inject.py

ë™ì‘:
  1. /tmp/cx-insight-store.dbì—ì„œ ë ˆì½”ë“œ ë¡œë“œ
  2. index.html ê¸°ì¡´ chatId ì¶”ì¶œ (ì¤‘ë³µ ì œê±°)
  3. ì‹ ê·œ ë ˆì½”ë“œë§Œ í•„í„°
  4. BigQueryì—ì„œ ì„¸ê·¸ë¨¼íŠ¸ ì¡°íšŒ (corp_number ì§ì ‘ ì‚¬ìš©)
  5. index.html ALL_RECORDSì— ì£¼ì…

ì¥ì :
  - Excel ì—†ì´ DB â†’ ëŒ€ì‹œë³´ë“œ ìë™í™”
  - corp_numberê°€ DBì— ì´ë¯¸ ìˆì–´ BQ ë§¤í•‘ ë¶ˆí•„ìš”
"""

import json
import os
import re
import sqlite3
import subprocess
import sys
from datetime import datetime

SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
ROOT_DIR = os.path.dirname(SCRIPT_DIR)
INDEX_HTML = os.path.join(ROOT_DIR, "index.html")
INSIGHT_DB = "/tmp/cx-insight-store.db"
SEGMENT_CACHE = os.path.join(ROOT_DIR, "data", "segment_cache.json")


# â”€â”€ í•œë„ êµ¬ê°„ ë§¤í•‘ â”€â”€

def to_limit_tier(grant_limit):
    if grant_limit is None:
        return "ë¯¸ë§¤ì¹­"
    try:
        v = float(grant_limit)
    except (ValueError, TypeError):
        return "ë¯¸ë§¤ì¹­"
    if v < 10_000_000:
        return "1ì²œë§Œ ë¯¸ë§Œ"
    elif v < 50_000_000:
        return "1ì²œë§Œ~5ì²œë§Œ"
    elif v < 100_000_000:
        return "5ì²œë§Œ~1ì–µ"
    elif v < 500_000_000:
        return "1ì–µ~5ì–µ"
    else:
        return "5ì–µ ì´ìƒ"


# â”€â”€ DB ë¡œë“œ â”€â”€

def load_db_records():
    if not os.path.exists(INSIGHT_DB):
        print(f"âŒ DB ì—†ìŒ: {INSIGHT_DB}")
        print("  cx-insight-collector.pyë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
        sys.exit(1)

    conn = sqlite3.connect(INSIGHT_DB)
    conn.row_factory = sqlite3.Row
    rows = conn.execute("""
        SELECT id, date, tags, tag_major, impact, journey,
               title, summary, customer_request, result,
               slack_ts, slack_link, corp_number, companyname, assignee
        FROM insights
        ORDER BY date ASC
    """).fetchall()
    conn.close()
    return [dict(r) for r in rows]


# â”€â”€ HTMLì—ì„œ ê¸°ì¡´ chatId ì¶”ì¶œ â”€â”€

def load_existing_chat_ids():
    if not os.path.exists(INDEX_HTML):
        return set()
    with open(INDEX_HTML, "r", encoding="utf-8") as f:
        content = f.read()
    match = re.search(r"const ALL_RECORDS = (\[.*?\]);", content, re.DOTALL)
    if not match:
        return set()
    records = json.loads(match.group(1))
    return set(r.get("chatId", "") for r in records)


def load_existing_records():
    if not os.path.exists(INDEX_HTML):
        return [], ""
    with open(INDEX_HTML, "r", encoding="utf-8") as f:
        content = f.read()
    match = re.search(r"const ALL_RECORDS = (\[.*?\]);", content, re.DOTALL)
    if not match:
        return [], content
    return json.loads(match.group(1)), content


# â”€â”€ BQ ì„¸ê·¸ë¨¼íŠ¸ ì¡°íšŒ â”€â”€

def load_segment_cache():
    if not os.path.exists(SEGMENT_CACHE):
        return {}
    with open(SEGMENT_CACHE, "r", encoding="utf-8") as f:
        rows = json.load(f)
    seg_map = {}
    for row in rows:
        cid = str(row.get("corp_id", ""))
        seg_map[cid] = {
            "detailCategory": row.get("detail_category") or "ë¯¸ë§¤ì¹­",
            "stabilityStatus": row.get("stability_status") or "ë¯¸ë§¤ì¹­",
            "growthStatus": row.get("growth_status") or "ë¯¸ë§¤ì¹­",
            "limitTier": to_limit_tier(row.get("grant_limit")),
        }
    return seg_map


def fetch_segments_from_bq(corp_ids):
    if not corp_ids:
        return load_segment_cache()

    id_list = ", ".join(str(c) for c in corp_ids)
    query = f"""
    SELECT corp_id, detail_category, stability_status, growth_status, grant_limit
    FROM `gowid-prd.mart_customer_segment.segment_base`
    WHERE month_id = (SELECT MAX(month_id) FROM `gowid-prd.mart_customer_segment.segment_base`)
      AND corp_id IN ({id_list})
    """
    cmd = ["bq", "query", "--use_legacy_sql=false", "--format=json", "--max_rows=10000", query]

    try:
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=30)
        if result.returncode == 0:
            rows = json.loads(result.stdout)
            seg_map = {}
            for row in rows:
                cid = str(row.get("corp_id", ""))
                seg_map[cid] = {
                    "detailCategory": row.get("detail_category") or "ë¯¸ë§¤ì¹­",
                    "stabilityStatus": row.get("stability_status") or "ë¯¸ë§¤ì¹­",
                    "growthStatus": row.get("growth_status") or "ë¯¸ë§¤ì¹­",
                    "limitTier": to_limit_tier(row.get("grant_limit")),
                }
            # ìºì‹œ ì—…ë°ì´íŠ¸
            with open(SEGMENT_CACHE, "w", encoding="utf-8") as f:
                json.dump(rows, f, ensure_ascii=False, indent=2)
            print(f"  BQ ì¿¼ë¦¬ ì„±ê³µ â†’ ìºì‹œ ì—…ë°ì´íŠ¸")
            return seg_map
    except (FileNotFoundError, subprocess.TimeoutExpired, json.JSONDecodeError):
        pass

    print(f"  BQ ì¿¼ë¦¬ ì‹¤íŒ¨ â†’ ìºì‹œ ì‚¬ìš©")
    return load_segment_cache()


# â”€â”€ DB ë ˆì½”ë“œ â†’ ALL_RECORDS í¬ë§· ë³€í™˜ â”€â”€

def convert_record(row, seg_map):
    corp_number = re.sub(r"[^0-9]", "", row.get("corp_number") or "")
    seg = seg_map.get(corp_number, {})

    return {
        # ê¸°ì¡´ í¬ë§· í˜¸í™˜ í•„ë“œ
        "chatId": row["slack_ts"],           # slack_tsë¥¼ chatIdë¡œ
        "date": row["date"],
        "company": row.get("companyname") or "ë¯¸í™•ì¸",
        "oldTag": row.get("tags") or "ë¯¸ë¶„ë¥˜",
        "primaryTag": row.get("tag_major") or "ë¯¸ë¶„ë¥˜",
        "secondaryTag": row.get("impact"),    # L1/L2/L3
        "confidence": "auto",                 # Claude ìë™ íƒœê¹…
        "detailCategory": seg.get("detailCategory", "ë¯¸ë§¤ì¹­"),
        "stabilityStatus": seg.get("stabilityStatus", "ë¯¸ë§¤ì¹­"),
        "growthStatus": seg.get("growthStatus", "ë¯¸ë§¤ì¹­"),
        "limitTier": seg.get("limitTier", "ë¯¸ë§¤ì¹­"),
        "firstAnswerSec": None,
        "closeSec": None,
        # DB ì „ìš© ì¶”ê°€ í•„ë“œ
        "impact": row.get("impact"),
        "journey": row.get("journey"),
        "title": row.get("title"),
        "summary": row.get("summary"),
        "customerRequest": row.get("customer_request"),
        "result": row.get("result"),
        "assignee": row.get("assignee"),
        "slackLink": row.get("slack_link"),
        "source": "db",                       # ë°ì´í„° ì¶œì²˜ êµ¬ë¶„
    }


# â”€â”€ HTML ì£¼ì… â”€â”€

def inject_into_html(all_records):
    with open(INDEX_HTML, "r", encoding="utf-8") as f:
        content = f.read()

    json_str = json.dumps(all_records, ensure_ascii=False, separators=(",", ":"))
    new_line = f"    const ALL_RECORDS = {json_str};"
    content = re.sub(
        r"    const ALL_RECORDS = \[.*?\];",
        new_line,
        content,
        count=1,
        flags=re.DOTALL,
    )

    # ìµœì¢… ì—…ë°ì´íŠ¸ ë‚ ì§œ
    all_dates = [r.get("date") for r in all_records if r.get("date")]
    if all_dates:
        max_date = max(all_dates)
        dt = datetime.strptime(max_date, "%Y-%m-%d")
        date_display = f"{dt.year}. {dt.month}. {dt.day}."
        content = re.sub(
            r'id="lastUpdateDate">[^<]+<',
            f'id="lastUpdateDate">{date_display}<',
            content,
        )

    with open(INDEX_HTML, "w", encoding="utf-8") as f:
        f.write(content)


# â”€â”€ ë©”ì¸ â”€â”€

def main():
    print("ğŸš€ Phase 3: DB â†’ ëŒ€ì‹œë³´ë“œ ìë™ ì£¼ì…\n")

    # 1. DB ë¡œë“œ
    print("[1/5] DB ë¡œë“œ")
    if not os.path.exists(INSIGHT_DB):
        print(f"  âŒ DB ì—†ìŒ. cx-insight-collector.py ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
        sys.exit(1)
    db_records = load_db_records()
    print(f"  DB ë ˆì½”ë“œ: {len(db_records)}ê±´ ({db_records[0]['date']} ~ {db_records[-1]['date']})")

    # 2. ê¸°ì¡´ chatId ì¶”ì¶œ (ì¤‘ë³µ ì œê±°)
    print("[2/5] ê¸°ì¡´ ë°ì´í„° ë¡œë“œ + ì¤‘ë³µ ì œê±°")
    existing_records, _ = load_existing_records()
    existing_ids = set(r.get("chatId", "") for r in existing_records)
    print(f"  ê¸°ì¡´: {len(existing_records)}ê±´, ê¸°ì¡´ ID: {len(existing_ids)}ê°œ")

    new_rows = [r for r in db_records if r["slack_ts"] not in existing_ids]
    print(f"  ì‹ ê·œ: {len(new_rows)}ê±´")

    if not new_rows:
        print("  ìƒˆ ë ˆì½”ë“œ ì—†ìŒ. ìµœì‹  ìƒíƒœì…ë‹ˆë‹¤.")
        return

    # 3. BQ ì„¸ê·¸ë¨¼íŠ¸ ì¡°íšŒ
    print("[3/5] BQ ì„¸ê·¸ë¨¼íŠ¸ ì¡°íšŒ")
    corp_ids = set()
    for r in new_rows:
        corp = re.sub(r"[^0-9]", "", r.get("corp_number") or "")
        if corp:
            corp_ids.add(int(corp))
    print(f"  ê³ ìœ  ì‚¬ì—…ìë²ˆí˜¸: {len(corp_ids)}ê°œ")
    seg_map = fetch_segments_from_bq(list(corp_ids))
    print(f"  ì„¸ê·¸ë¨¼íŠ¸ ë§¤ì¹­: {len(seg_map)}ê°œ ë²•ì¸")

    # 4. í¬ë§· ë³€í™˜
    print("[4/5] í¬ë§· ë³€í™˜")
    converted = [convert_record(r, seg_map) for r in new_rows]
    seg_matched = sum(1 for r in converted if r["detailCategory"] != "ë¯¸ë§¤ì¹­")
    print(f"  ì„¸ê·¸ë¨¼íŠ¸ ì£¼ì…: {seg_matched}/{len(converted)}ê±´")

    # 5. HTML ì£¼ì…
    print("[5/5] index.html ì£¼ì…")
    all_records = existing_records + converted
    inject_into_html(all_records)
    print(f"  âœ… ì´ {len(all_records)}ê±´ (ê¸°ì¡´ {len(existing_records)} + ì‹ ê·œ {len(converted)})")

    print(f"\n  ë‹¤ìŒ: git commit + push â†’ GitHub Pages ë°˜ì˜")
    print(f"  git -C {ROOT_DIR} add index.html && git -C {ROOT_DIR} commit -m 'chore: DB ë°ì´í„° {len(converted)}ê±´ ì¶”ê°€' && git -C {ROOT_DIR} push")


if __name__ == "__main__":
    main()
